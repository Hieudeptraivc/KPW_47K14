{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     all   an  anger_symbol  anger_symboltóm_lại  anh  any  back  bao  \\\n",
      "0    0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "1    0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "2    0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "3    0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "4    0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "..   ...  ...           ...                  ...  ...  ...   ...  ...   \n",
      "213  0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "214  0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "215  0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "216  0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "217  0.0  0.0           0.0                  0.0  0.0  0.0   0.0  0.0   \n",
      "\n",
      "     bao_nhiêu   be  ...  ướt_sũng  ảnh  ấn_tượng   ốc   ốm        ốp   ồm  \\\n",
      "0          0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
      "1          0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
      "2          0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
      "3          0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
      "4          0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
      "..         ...  ...  ...       ...  ...       ...  ...  ...       ...  ...   \n",
      "213        0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.312943  0.0   \n",
      "214        0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
      "215        0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
      "216        0.0  0.0  ...       0.0  0.0  0.088664  0.0  0.0  0.000000  0.0   \n",
      "217        0.0  0.0  ...       0.0  0.0  0.000000  0.0  0.0  0.000000  0.0   \n",
      "\n",
      "      ổn   ớt   ức  \n",
      "0    0.0  0.0  0.0  \n",
      "1    0.0  0.0  0.0  \n",
      "2    0.0  0.0  0.0  \n",
      "3    0.0  0.0  0.0  \n",
      "4    0.0  0.0  0.0  \n",
      "..   ...  ...  ...  \n",
      "213  0.0  0.0  0.0  \n",
      "214  0.0  0.0  0.0  \n",
      "215  0.0  0.0  0.0  \n",
      "216  0.0  0.0  0.0  \n",
      "217  0.0  0.0  0.0  \n",
      "\n",
      "[218 rows x 1096 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tu_dien = {}\n",
    "file_path = 'D:/học/Khai pha web/BTCB/choa.txt' \n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word, abbreviation = line.strip().split('\t')\n",
    "        tu_dien[word.strip()] = abbreviation.strip()\n",
    "f = open(\"D:/học/Khai pha web/BTCB/vietnamese-stopwords.txt\", \"r\", encoding=\"utf-8\")\n",
    "#Get Stop words Dictionaries\n",
    "List_StopWords=f.read().split(\"\\n\")\n",
    "src='D:/học/Khai pha web/Data1'\n",
    "path=os.listdir(src)\n",
    "line=\"\"\n",
    "for i in path:\n",
    "    label=i.split(\"'\")[0]\n",
    "    f=open('D:/học/Khai pha web/Data1/'+str(label), \"r\", encoding=\"utf-8\")\n",
    "    text=f.read()\n",
    "    # Loại bỏ thẻ HTML\n",
    "    mau_url = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Thay thế tất cả các URL trong chuỗi bằng chuỗi rỗng\n",
    "    text_pre = re.sub(mau_url, '', text)\n",
    "    text_pre=emoji.demojize(text_pre)\n",
    "    text_pre=text_pre.lower() # Convert text to lowercase\n",
    "    text_pre=re.sub(r'[^\\w\\s.]','',text_pre) # Remove punctuation\n",
    "    text_pre = re.sub(\"\\d+\", \" \", text_pre) # Remove number\n",
    "    text_pre=text_pre.replace(\"\\n\",\"\")  # Remove the newline command\n",
    "    text_pre = \" \".join(text_pre for text_pre in text_pre.split() if text_pre not in List_StopWords)\n",
    "    text_pre = re.sub(r\"[!@#$[]()]'\", \"\", text_pre) # Remove character: !@#$[]()\n",
    "# Get Abbreviations Words\n",
    "    text_pree=\"\"\n",
    "    words = text_pre.split()\n",
    "    for word in words:\n",
    "        w=word\n",
    "        w = re.sub(r'[^\\w\\s]','',w) #Removing Punctuation\n",
    "        if w.lower() in tu_dien:\n",
    "            word=tu_dien[w]\n",
    "        text_pree=text_pree + \" \" + word \n",
    "    line+=text_pree\n",
    "Dataa=line.split(' . ')\n",
    "Data=[]\n",
    "for x in Dataa:\n",
    "    if x.strip() != '':\n",
    "        Data.append(x)\n",
    "\n",
    "tr_idf_model  = TfidfVectorizer()\n",
    "tf_idf_vector = tr_idf_model.fit_transform(Data)\n",
    "\n",
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "# print(tf_idf_array)\n",
    "\n",
    "W = tr_idf_model.get_feature_names_out()\n",
    "# print(W)\n",
    "\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = W)\n",
    "print(df_tf_idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
